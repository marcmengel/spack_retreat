## Requirements

1. It must be easy for an experiment to move many scripts from one "version" to another "version"

* stakeholders: DAQ, production, analysts

2.  Users must be able to access a stable base of software, and extend it with his own builds of higher-level packages.

* stakeholders: developers


## Terminology


## Various concerns, issues and questions

90% or more of the time spent on spack has gone into supporting builds for SL7.
SL7 is too old for spack to work well.

Eric points out that ROOT is in the official spack repo, not in the FNAL repo.
Can we decrease our support load by *not* considering ROOT to be a "CSAID-supplied package"?

Can we eventually consider dropping binary installations, and rather merely deliver YAML files/recipes.

We need reference builds of ... which packages? art, ots_daq

We should think of what requirements we want to place on a "spack release" before it becomes a "Fermi spack release".
We need to "vet" the releases from the spack team to verify they are workable for us.
Marc M has an example of a case when this was *not* automatic.
Part of our solution may be long-term support of our own fork of spack.

There is consensus that CSAID would not save effort by discontinuing the production of binary releases made available to the experiments.
This would result in a small savings of effort in the creation and distribution of some builds, and an larger increase in the support load because we are no longer carefully controlling the software mixes used by the experiments.

We may want to have a definition of the "current" version of any (sone or all) released software.
Marc M suggests doing this through the use of symbolic links to point to a specific version of an environment.
This is what UPS currently allows.
Do experiments make use of this now?
If not, are they doing it because they are unaware of it, or because it does not meet their needs?
    * Experiments avoid this for releases of their own software
Do any projects use this?
    * The DAQ efforts avoid the use of curret because it is not sufficiently precise.
The concept of "current" is useful for tools like gdb, valgrind, mrb, cmake, ifdhc_config: things upon which our code doesn't have binary dependencies.
Users need to be able to conveniently get the list of available versions of a package or an environment.
    * the suite configuration server needs to be interrogatable (browsable) to get this information.
We want the ability to have an existing experiment script pick up a brand new ifdhc_config, without changing the script.
    * spack's ability to deprecate a package version may handle this

Experiments need to know how to make a release (environment, bundle package, whatever we decide).
    * if they use our tools, they will have documentation and our support
    * if they roll their own, they are on their own

Experiments need to know how to package products (implying all the steps involved).

How do we distribute environments/releases?

We need to complete enough documentation for experiments to be able to move to the new system.

How do we use spack in a grid job?

How do we reduce the number of environments and specifications we create for products and environments shared across experiments?
Rephrase: we want to create as few build instances of lower-level packages.
Rephrase: how do we prevent unnecessary duplication of built lower-level products?
(set-theory union of requirements)
    * proposed policy: for products like Boost, which have many variants, and which are low level and broadly used, we should build the batteries-included version. Enable as many flags as possible without giving us build problems.
    * can we have a policy on the definition of the grid environment so that we can require that system things like X libraries, ssl, etc., are required in the grid environment -- even if they are not going to be used in the grid environment -- so that we can make fewer variants of our packages which depend upon them?
    * we can use spack environments, or views, or ... to make it easy for "users" to "set up" the right environment for their use, in conjunction with making a small multiple of builds, e.g. with and without X?
    * we need to carefully vet all pull requests sent to CSAID managed spack recipe repositories: this is also a continuous (low-level?) effort.

Managing the use of X, or Qt, or GL, or ...:
    * we have multiple options, e.g. have multiple distinct builds handled with spack, or demand an underlying "system environment/container image" that hhas the set of libraries that we require to be present.


How do we manage package-recipe repositories?
What is the right number of such repositories, and how are they related?

    * We need at least one CSAID-maintained spack recipe repository.
    * It is also possible to have a tree (not more general graph) of dependent repositories, mirroring in structure the dependencies of the software suite dependencies (e.g. critic -> nutools -> larsoft).
    * Each experiment should maintain its own spack recipe repository.
    * We propose each GitHub organization have its own package-recipe repository.


How do we cope with different spack versions?
    * maintain a FNAL spack fork in perpetuity
    * provide migration abilities for (e.g.) buildcaches

How do we keep up with changes upstream?
How do we get our changes to spack merged upstream?
    * we need to maintain an active particpation in the spack developer community; we can not be passive users
    * forward-looking; not an emergent issue

What UPS support do we provide for transition to spack on SL7?

    * We can provide very limited support of the existing UPS -> spack translation tools, and only on special requests that will be approved on a case-by-case basis.

How do users release managers test new software versions?
    * The release manager needs to test experiment software in branches of the experiment source code repositories (CI)
    * spack develop @ particular commit 
    * spack recipes need to be adjusted to do this correctly
    * github actions to support the use might be needed

Can we relax the level of specificity we require on compiler versions to take advantage of binary compatibility promises provided by compiler vendors?

Can we make our burden lighter by some reduction in the specificity of variant/version/etc. we define for each product and suite?

## Use cases for different types of stakeholders

1. **developer use case**: work on a new version of some part of the software stack, using lower-level packages that are pre-built and installed somewhere.
This work is done in a development environment.

2. **production use case**: use an installed code to run batch jobs at large scale.

3. **analyst use case**: take newly developed and thus not yet released code to run grid jobs.

4. **experiment release manager use case**: take a set of tagged source code and build and distribute a new experiment release.

5. **CSAID release manager use case**: take a set of tagged source code and build and distribute a new CSAID release.
